
================
16/10/2023:

UPDATE:
(1) Euler Cluster: could discuss with ETH supervisor about requesting more resources
(2) start of model implementations; currently stuck with static models, more time needed for SSMs
(3) reading about RC (how to implement neural models), Bayesian model assessment

QUESTIONS:
(1) IBIS samples parameters outside prior support, i.e. constraints --> in move/MCMC step, how possible that particles with 0 prior prob accepted?
--> should indeed not happen; fix: enforce non-negativity by exponentiating; transforming parameter space can generally improve efficiency
(2) Bootstrap vs Guided vs Auxiliary PF for StochVol? 'waste-free'?
--> in StochVol models, observation not very informative about volatility, hence bootstrap should work OK; guided may well improve it, worth looking into; auxiliary most likely not worth it; wastefree might improve slightly, worth looking into whether there are cons/risks with it (like with auxiliary filter)

NEXT STEPS:
(1) create github repo
(2) check issue in MH move step
(3) get dynamic models running
(4) ask BÃ¼hlmann re Cluster share


================
09/10/2023:

UPDATE:
(1) reading about SSMs, PFs, SMC, IBIS, SMC^2

QUESTIONS:
(1) Example code for GARCH/SV?
--> will send (but: don't try to rush to get things working; focus on understanding what's happening)
(2) Continuous-time models: makes sense to discretize and incorporate in comparison?
--> Yes
(3) Transform to log first (because made for raw process), but drift term remains; e.g. GARCH has mean zero?
--> Transform to log-process, then get rid of drift term (equivalent martingale measure, like in options pricing); then e.g. GBM = white noise (simplest possible model)
(5) Does time increment matter in discrete-models? (e.g. if "t"-"t-1" = 1 day or 1 microsecond)
--> model performance impacted by time scale, e.g. SDEs work better on large scales (because on very small scales process is just jumps i.e. further away from continuous); for start work with daily data; intra-day data might make sense to compute realized variance to assess volatility forecasts later
(6) What prior to ensure fair comparison for model selection? Theory? Run multiple times, check sensitivity?
--> not much theory; in practice run sensitivity analysis, i.e. run algorithm for same model with different prior and check difference in evidence

NEXT STEPS:
(1) familiarize with (i) OOP and (ii) 'particles' package (properly! don't rush to get results)
(2) read up on Bayesian Data Analysis (Garmin), alternative criteria BIC/WAIC/Deviance IC
(3) think about how to assess volatility predictions, check literature
(4) check how to get more cores on ETH Euler Cluster (non-guest access)


================
02/10/2023:

No meeting


================
25/09/2023:

UPDATE:
(1) checked OOP material from Berkeley; exercises on Datacamp;
(2) started implementing GARCH in Python;
(3) checked material on computational methods from course from Imperial, started watching lecture videos;
(4) bit of research on neural models, RC, & Signatures, get ideas how could be useful further down the road;

QUESTIONS:
(1) Neural models:
(i) calibration: use options price data vs. asset price data?
--> could be interesting, but: not probabilistic anymore; still: could maybe combine, make model replicate option prices, then assess fit to asset price data

(ii) NSVM: inherently intractable; reservoir -> linear -> tractable?
--> Yes, should be tractable

(2) RandSig reservoir: reconstruct volatility functional from readout coefficients? If yes, estimate driving Brownian motion of empirical data? Seen Signature-based volatility models before (for path-dependence)?
--> could work, but hard to reconstruct volatility function; idea: try with normal signature (see Signatory by Kidger), because then know what coefficients correspond to, then repeat for different starting values to reconstruct V_1 ... V_k (would have to write down carefully); reconstructing Brownian motion from data not easy (see Cuchiero 2023)

(3) Packages in R/Python for GARCH (rugarch, PyFlux, Markov-switching) all based on MCMC (adaptive); Why not SMC?
--> MCMC potentially better for computing posterior, but SMC best for computing marginal likelihood (i.e. for model selection) and only applicable one for online inference (important in finance)

NEXT STEPS:
(1) read up on...
 - SMC (samplers)
 - Bayes factor


================
18/09/2023:

UPDATE:
(1) Data: ETH has licenses for Bloomberg & Capital IQ Pro; supervisor noted Stanford page for datasets of papers & Kaggle dataset

QUESTIONS:
(1) How project related to your own work? Which extensions/applications interested in most (many models, volatility forecasting, portfolios, hedging, ...)?
--> Chris benefits from supervision of MSc thesis per se, no strict expectations for results

(2) Which extension to focus on first?
--> no preference, will develop with time

(3) How can neural volatility models be estimated in Bayesian way?
--> Reservoir Computing: random weights for early layers (not trained!), only readout layer trained; ~5 parameters sufficient; Phil not aware of smart ways to set random weights for purposes of volatility modelling

(4) How often to meet? Jour fixe?
--> Monday, 6pm CH time

NEXT STEPS:
(1) Read up on ... (Chris sends material)
 - GARCH & other volatility models (likelihood fct, ...);
 - Sequential Monte Carlo;
 - Object-Oriented Programming

(2) Implement estimation of simple models, first MLE, then SMC