\chapter{Reservoir Computing} 
\label{sec:RC}

\section{Random Neural Networks}

\emph{random feature models},

\emph{Extreme Learning Machines} (ELMs). An ELM is a shallow feedforward neural network with randomly drawn inner weights. In the one-dimensional case, we have
\begin{align*}
\textrm{ELM}(\pmb{x}) ~ :=& ~ \pmb{\beta}^\top \phi(\pmb{Ax} + \pmb{b}) + \beta_0 \\
\pmb{A}_{ij}, \pmb{b}_i ~ \simiid & ~ 
\end{align*}

\emph{echo state network}




\section{Signature Methods}

\begin{definition}[Signature]
Let $\pmb{u} \in BV([0,T]; \R^p)$. The signature component associated with the word $w \in \{0, ..., d\}^k$ of length $k \geq 0$ is
\begin{equation*}
\Sig^{(w)}(\pmb{u}) ~ := ~ \int_{0 \leq t_1 \leq \cdots \leq t_k \leq t} \d u^{(i_1)}_{t_1} \cdots \d u^{(i_k)}_{t_k}
\end{equation*}
\end{definition}

Signature of time-extended path is point-separating. 

Continuous path functionals can be uniformly approximated on compact sets by linear functionals of the time-extended signature evaluated at the final time

\begin{theorem}[\citep{Cuchiero22}]
Let $\pmb{u}$ be a càdlàg path and $f$ a continuous function on path space. Then, for every $\varepsilon > 0$, there exists a linear functional $\ell$ such that
\begin{equation*}
\sup_{\pmb{u} \in K} \big|f(\pmb{u}) - \ell(\Sig(\pmb{u})) \big| ~ \leq ~ \varepsilon
\end{equation*}
\end{theorem}

Hence signature effective feature extraction method for time series and well suited to capture path-dependency.
\begin{align*}
X_t ~ &= ~ \sigma_t Z_t \\
\sigma_t ~ &= ~ \beta_0 + \pmb{\beta}^\top \Sig(X_{1:t})
\end{align*}

Truncated signature
\begin{equation*}
\Sig^{M}(\pmb{u}) ~ := ~ \int_{0 \leq t_1 \leq \cdots \leq t_k \leq t} \d u^{i_1}_{t_1} \cdots \d u^{i_k}_{t_k}
\end{equation*}

Randomized signature
\begin{theorem}[\citep{Cuchiero21}]
Consider random matrices $\pmb{A}_i \in \R^{q \times q}$, random vectors $\pmb{b}_i \in \R^q$, and activation function $\phi: \R \to \R$ (applied component-wise). The random vector $\widehat{\Sig} \in \R^q$ constructed as the solution to the controlled ODE
\begin{equation*}
\d \widehat{\Sig}_t ~ = ~ \sum_{i=1}^d \phi \Big( \pmb{A}_i \widehat{\Sig}_t + \pmb{b}_i \Big) \d u_t^{(i)}
\end{equation*}
with random initial condition $\widehat{\Sig}_0$ approximately preserves the geometry of $\Sig$, in the sense that
\begin{equation*}
(1-\varepsilon) \norm{\Sig(\pmb{u}_t) - \Sig(\pmb{u}_t)}_2 ~ \leq ~ \norm{\RandSig(\pmb{u}) - \RandSig(\pmb{u}')}_2 ~ \leq ~ (1+\varepsilon) \norm{\Sig(\pmb{u}) - \Sig(\pmb{u})}_2
\end{equation*}
\end{theorem}
