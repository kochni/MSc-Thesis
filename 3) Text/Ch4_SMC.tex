\chapter{Sequential Monte Carlo Methods} 

If $\mathcal{V}$ was finite, the posterior is similarly tractable and can be computed following a procedure known as the Baum-Welch algorithm.

\section{Basics of Monte Carlo Estimation}

Consider a random variable $\theta \in \Theta$ distributed according to (cumulative) distribution function $\pi$. On the one hand, we are interested in the expectation of a test function $f: \Theta \to \R^d$,
\begin{align*}
\E_\pi[f(\theta)] ~ = ~ \int_\Theta f(\theta) \d \pi(\theta)
\end{align*}
Depending on $\Theta$, $f$, and $\pi$, this integral quickly becomes impossible to solve. The central dogma of Monte Carlo estimation is to exploit a approximation of $\pi$ based on individual samples, or "particles" therefrom, giving rise to its particle approximation, or empirical distribution function
\begin{equation*}
\hat{\pi}(\theta) ~ = ~ \frac{1}{n} \sum_{i=1}^n 1(\theta \leq \theta_i)
\end{equation*}
Since $\hat{\pi}$ is piecewise constant and always jumps by $\frac{1}{n}$, such a particle approximation of $\pi$ reduces the problem to a much more tractable summation task, namely
\begin{equation*}
\E_{\hat{\pi}}[f(\theta)] ~ = ~ \int_\Theta f(\theta) \d \hat{\pi}(\theta) ~ = ~ \frac{1}{n} \sum_{i=1}^n f(\theta_i)
\end{equation*}
This basic idea can be applied in any setting where distributions can be approximated by particle approximations and test functions can be evaluated pointwise. For instance, under a Bayesian paradigm, we are interested in the \emph{evidence} of a model given some data $x$, which is defined as
\begin{equation*}
p(x) ~ = ~ \E_\pi[p(x \mid \theta)] ~ = ~ \int_\Theta p(x \mid \theta) \d \pi(\theta)
\end{equation*}

\begin{equation*}
\widehat{p}(x) ~ = ~ \frac{1}{n} \sum_{i=1}^n p(x \mid \theta_i)
\end{equation*}
i.e. the average likelihood of the data over the sampled parameters.


\section{Importance Sampling}

Consider the following trivial identities:
\begin{equation*}
\pi(\theta) ~ = ~ \int_{\theta' \leq \theta} \d \pi(\theta') 
	= ~ \int_{\theta' \leq \theta} \frac{\d \pi(\theta')}{\d \nu(\theta')} \d \nu(\theta') ~ = ~ \int_{\theta' \leq \theta} w(\theta') \d \nu(\theta')
\end{equation*}
and hence for $x_i \simiid q$, $i=1,...,n$,
\begin{equation*}
\hat{\pi}(\theta) ~ = ~ \frac{1}{N} \sum_{i=1}^N w(\theta_i) 1(\theta \leq \theta_i)
\end{equation*}
where $\nu$ is another distribution function called the "proposal" and $w(\theta) := \frac{\d \pi(\theta)}{\d \nu(\theta)}$ is the Radon-Nikodym derivative of $\pi$ w.r.t. $\nu$. This suggests that if it is not possible to sample from $\pi$, but from $\nu$ and to evaluate $w(\theta)$ pointwise, we can still use Monte Carlo techniques to estimate quantities of $\pi$.

The weight $w(\theta)$ corrects for the fact that $\theta$ was drawn from the wrong distribution. Intuitively, if a specific particle $\theta_i$ is common under the target $\pi$ but rare under the proposal $\nu$, it needs to be given high weight, and vice versa.

Formally, the only requirement is that $w$ is always well-defined, i.e. that $\sup_\theta \frac{\d \pi(\theta)}{\d \nu(\theta)} \leq M$ for some $M < \infty$. This in turn requires that $\d \nu(\theta) = 0$ only when $\d \pi(\theta) = 0$, implying that the support of $\nu$ must contain the support of $\pi$. Additionally, $\nu$ must have fatter tails than $\pi$ (if the latter's support is unbounded). Otherwise, if e.g. the density of $\nu$ decayed more rapidly towards zero than that of $\pi$, $w$ would of course grow indefinitely.

In many practical applications, $\pi$ is only known up to a multiplicative factor. That is, we know $\gamma(\theta)$ such that
\begin{equation*}
\frac{1}{Z} \gamma(\theta) ~ = ~ \pi(\theta)
\end{equation*}
where $Z := \int_\Theta \d \gamma(\theta)$ is an unknown normalizing constant ensuring that $\pi(\theta) \tendsto{\theta \to \infty} 1$. In this case, one has to work with the unnormalized weights $W(\theta) := \frac{\d \gamma(\theta)}{\d \nu(\theta)}$. It turns out that these are unbiased estimates of the unknown normalizing constant $Z$:
\begin{align*}
\E_\nu \Big[ W(\theta) \Big] ~ &= ~ \int_\Theta W(\theta) \d \hat{\pi}(\theta) ~ = ~ \int_\Theta \d \gamma(\theta) ~ =: ~ Z
\end{align*}
suggesting use of the estimator
\begin{equation*}
\widehat{Z} ~ := ~ \frac{1}{N} \sum_{i=1}^N W(\theta_i)
\end{equation*}
In the context of model selection, $Z$ is itself of prime interest, and hence this is a relevant result.
\begin{equation*}
\V_\nu(\widehat{Z}) ~ = ~ \frac{1}{N} \V_\nu \big( W(\theta) \big)
\end{equation*}
Since, if $\nu = \pi$, the unnormalized weights are always $1$, and hence $\widehat{Z}$ has zero variance, it is clear that the optimal proposal should be as close to the target as possible.
\begin{equation*}
\nu^* ~ = ~ \pi
\end{equation*}

\begin{align*}
\hat{\pi}(\theta) ~ &= ~ \frac{1}{n} \sum_{i=1}^N \frac{W(\theta_i)}{\sum_{i=1}^n W(\theta_i)} 1(\theta \leq \theta_i)
\end{align*}

\begin{equation*}
\ESS ~ := ~ \frac{N}{1 + \V_\nu(W(\theta))}
\end{equation*}


\section{Sequential Importance Sampling}

Suppose our unknown quantity $\theta$ is a parameter in the distribution $p(x)$ of $x$. Observing data $x_{1:t} = \{x_1,...,x_t\}$ carrying information about this quantity gives rise to the target distribution $\pi_t(\theta)$.

If all the observations $x_{1:t}$ arrive simultaneously, batch inference is a popular approach. Therein, one attempts to estimate the distribution by computing resp. approximating $\pi_t$ directly. Some of the most popular batch inference methods include Markov chain Monte Carlo (MCMC) and Hamiltonian Monte Carlo (HMC).

When a new observation $x_{t+1}$ arrives, however, batch inference methods have no direct way to use $\pi_t$. Consequently, when data arrives sequentially, one is left with no other choice than running the entire analysis again every time. This is a severe limitation in fields including finance, where often data arrives and hence models have to be updated daily if not more frequently.

In sequential Bayesian inference, the idea is to construct intermediate distributions and sequentially update them to incorporate new observations. That is, they are inherently on-line by construction. This is of primary interest when a model needs to be estimated today but new data will inevitably arrive tomorrow. It has also been demonstrated to have attractive properties in batch inference.

\begin{equation*}
X_t \mid (X_{t-1} = x_{t-1}, \theta) ~ \sim ~ P_\theta(\cdot \mid x_{t-1})
\end{equation*}
where $P_\theta(\cdot) = P(\cdot \mid \theta)$.
\begin{equation*}
\pi_t(\theta) ~ := ~ \pi(\theta \mid x_{1:t})
\end{equation*}

\begin{align*}
\pi_t(\theta) ~ &= ~ \frac{1}{Z_t} \gamma_t(\theta); \quad Z_t ~ = ~ \int_\Theta \d \gamma_t(\theta) \\
\d \gamma_t(\theta) ~ &= ~ P_\theta(x_{1:t}) \d \pi_0(\theta) ~ = ~ \d \pi_0(\theta) \cdot \prod_{k=1}^t P_\theta(x_k \mid x_{k-1}) \\
\end{align*}
Through another trivial identity, we obtain a recursive expression for the weights, namely
\begin{align*}
W_t(\theta) ~ := ~ \frac{\d \gamma_t(\theta)}{\d \nu_t(\theta)} ~ &= ~ \frac{\d \gamma_{t-1}(\theta)}{\d \nu_{t-1}(\theta)} \cdot \frac{\d \gamma_t(\theta) \, / \d \gamma_{t-1}(\theta)}{\d \nu_t(\theta) \, / \d \nu_{t-1}(\theta)} \\
	&= ~ W_{t-1}(\theta) \cdot \alpha_t(\theta)
\end{align*}
where $\alpha_t(\theta) := \frac{\d \gamma_t(\theta) \, / \d \gamma_{t-1}(\theta)}{\d \nu_t(\theta) \, / \d \nu_{t-1}(\theta)}$ is the weight update from $t-1$ to $t$. If use a constant proposal distribution at each iteration, ignoring new information from the observations $x_{1:t}$, then $\alpha_t$ corresponds to the conditional likelihood of the $t^{\textrm{th}}$ observation
\begin{equation*}
\frac{\d \gamma_t(\theta)}{\d \gamma_{t-1}(\theta)} ~ = ~ P_\theta(x_t \mid x_{t-1})
\end{equation*}
An estimate for the normalizing constant is obtained akin to standard importance sampling
\begin{equation*}
\widehat{Z}_t ~ = ~ \frac{1}{n} \sum_{i=1}^N W_t(\theta_i)
\end{equation*}

\section{Resampling}

weight degeneracy

multinomial, stratified, systematic

\begin{algorithm}
\label{alg:IBIS}
\caption{Iterated Batch Importance Sampling (IBIS)}
\hspace*{\algorithmicindent} \textbf{Input} $\pi_0$, $P$, $K_t$, \texttt{resample} \\
\hspace*{\algorithmicindent} \textbf{Output} $\widehat{\pi}_t, \widehat{Z}_{1:t}$
\begin{algorithmic}
\State $\theta_0^{(i)} \sim \pi_0(\theta)$ \algorithmiccomment{generate particles from prior}
\State $W_0^{(i)} \setto 1$ \algorithmiccomment{initialize uniform weights}
\For{$t=1$ to $T$}
	\If{$\ESS < \ESSmin$}
		\State $\tilde{\theta}_{t-1}^{(i)} \setto \resample(\theta_{t-1}^{(1:N)}; w_{t-1}^{(1:N)})$ \algorithmiccomment{resample particles}
		\State $\widetilde{W}_{t-1}^{(i)} \setto 1$ \algorithmiccomment{reset weights}
	\Else
		\State $\tilde{\theta}_{t-1}^{(i)} \setto \theta_{t-1}^{(i)}$ \algorithmiccomment{maintain particles}
		\State $\widetilde{W}_{t-1}^{(i)} \setto W_{t-1}^{(i)}$ \algorithmiccomment{maintain weights}
	\EndIf
\State $\theta_t^{(i)} \sim K_t(\cdot \mid \widetilde{\theta}_{t-1}^{(i)})$ \algorithmiccomment{propagate particles}
\State $\alpha_t^{(i)} \setto P(x_t \mid x_{t-1}; \theta_t^{(i)})$ \algorithmiccomment{compute weight update}
\State $W_t^{(i)} \setto \widetilde{W}_{t-1}^{(i)} \cdot \alpha_t^{(i)}$ \algorithmiccomment{update weights}
\State $w_t^{(i)} \setto W_t^{(i)} / \sum_{j=1}^N W_t^{(j)}$ \algorithmiccomment{re-normalize weights}
\EndFor
\end{algorithmic}
\end{algorithm}

In the original paper, \citeauthor{Chopin02} \citeyearpar{Chopin02} proposes to use an adaptive MCMC scheme, wherein the target is approximated through a Gaussian distribution with the empirical mean $\hat{\mu}$ and covariance $\widehat{\Sigma}$ of the particles. New particles are then proposed via a Gaussian random walk, so that $K_t(\cdot \mid \theta) = \mathcal{N}(\theta, \widehat{\Sigma})$


\section{Particle Filtering}

In stochastic models, the evolution of $(X_t)$ depends that of another, latent stochastic process $(V_t)$. The models proposed in Sect. \ref{sec:models} are all of the form
\begin{align*}
X_t \mid (X_{t-1}=x_{t-1}, V_t=v_t) ~ &\sim ~ P(\cdot \mid x_{t-1}, v_t) \\
V_t \mid V_{t-1}=v_{t-1} ~ &\sim ~ K(\cdot \mid v_{t-1})	
\end{align*}
where $P$ is a likelihood function for the observations and $K$ is a transition kernel. In stochastic volatility and other models, $V_t$ can be interpreted as the current "state" of the system, hence such models are referred to as \emph{state-space models}. For the moment, suppose that $P$ and $K$ do not depend on any (unknown) parameters.

Our target distribution is that of the latent path $v_{1:t}$,
\begin{equation*}
\pi_t(v_{1:t}) ~ = ~ \frac{1}{Z_t} \prod_{j=1}^t K_\theta(v_j \mid v_{j-1}) P_\theta(x_j \mid x_{j-1}, v_j) ~ = ~ \frac{1}{Z_t} \gamma_t(v_{1:t})
\end{equation*}
where we define prior distributions $P(x_1 \mid x_0, v_1) := \eta_X(x_1 \mid v_1)$ and $K(v_1 \mid v_0) := \eta_V(v_1)$. From this distribution on the path space, we can deduce all distributions of interest through integration. These include the \emph{filtering distribution} $\pi_t(v_t \mid v_{1:t-1})$, evidence $p(x_{1:t}) = \int \pi_t(v_t \int)$

We can again readily extend the sequential importance sampling framework to approximate the new target distribution. Borrowing the expression from the static setting, we have the recursion for the weights
\begin{equation*}
W_t(v_{1:t}) ~ = ~ W_{t-1}(v_{1:t}) \cdot \frac{\d \gamma_t(v_t \mid v_{1:t-1})}{\d \nu_t(v_t \mid v_{1:t-1})}
\end{equation*}
where the numerator of the weight update is again just a conditional likelihood, this time of the new latent quantity $v_t$ given the previous ones, $v_{1:t-1}$, and the observations $x_{1:t}$. It is given by
\begin{equation*}
\d \gamma_t(v_t \mid v_{1:t-1}) ~ = ~ P(x_t \mid v_t) K(v_{1-t} \mid v_t)
\end{equation*}
This gives rise to the so called \emph{guided particle filter} outlined in Alg. \ref{alg:GPF}.

\begin{algorithm}
\label{alg:GPF}
\caption{(Guided) Particle Filter}
\hspace*{\algorithmicindent} \textbf{Input} $\eta_V$, $\eta_X$, $P$, $K$, $\ESSmin$, \texttt{resample}
\begin{algorithmic}
\State $v_1^{(i)} \sim \eta_V(v_1)$ \algorithmiccomment{generate particles from prior}
\State $W_1^{(i)} \setto 1$ \algorithmiccomment{initialize weights uniformly}
\State $w_1^{(i)} \setto N^{-1}$
\For{$t=2$ to $T$}
	\State $\ESS = 1 / \sum_{j=1}^N (W_t^{(i)})^2$ \algorithmiccomment{compute ESS}
	\If{$\ESS < \ESSmin$}
		\State $\widetilde{v}_{t-1}^{(i)} \sim \resample(v_{t-1}^{(1:N)}; w_{t-1}^{(1:N)})$ \algorithmiccomment{resample particles}
		\State $\widetilde{W}_{t-1}^{(i)} \setto 1$ \algorithmiccomment{reset weights}
	\Else
		\State $\widetilde{v}_{t-1}^{(i)} \setto v_{t-1}^{(i)}$ \algorithmiccomment{maintain particles}
		\State $\widetilde{W}_{t-1}^{(i)} \setto W_{t-1}^{(i)}$ \algorithmiccomment{maintain weights}
	\EndIf
\State $v_t^{(i)} \sim K(\cdot \mid \widetilde{v}_{t-1}^{(i)})$ \algorithmiccomment{propagate particles}
\State $\alpha_t^{(i)} \setto K(v_t^{(i)} \mid v_{t-1}^{(i)}) P(x_t \mid x_{t-1}, v_t^{(i)}) \, / \d \nu_t(v_t \mid v_{t-1})$ \algorithmiccomment{compute weight update}
\State $W_t^{(i)} \setto \widetilde{W}_{t-1}^{(i)} \cdot \alpha_t^{(i)}$ \algorithmiccomment{update weights}
\State $w_t^{(i)} \setto W_t^{(i)} / \sum_{j=1}^N W_t^{(j)}$ \algorithmiccomment{re-normalize weights}
\EndFor
\end{algorithmic}
\end{algorithm}

In particle filtering, unlike in IBIS, particles do not necessarily have to be moved after resampling to introduce variation, since they are propagated either way.

A problem that remains is that of the choice of proposal distribution. Akin to standard importance sampling, the optimal proposal is again the true law,
\begin{equation*}
\d \nu_t^*(v_t \mid v_{t-1}) ~ = ~ \pi_t(v_t \mid v_{t-1}) ~ = ~ \frac{P(x_t \mid v_t) K(v_t \mid v_{t-1})}{\int P(x_t \mid v_t) K(v_t \mid v_{t-1}) \d x_t}
\end{equation*}
However, this is often intractable in practice. A popular alternative is to ignore the information about $v_t$ brought by the $x_t$ and simply use $\nu_t(v_t \mid v_{t-1}) = K(v_t \mid v_{t-1})$. This is referred to as the \emph{bootstrap proposal}. Naturally, its inefficiency is smaller whenever $x_t$ not very informative about $v_t$.


\section{SMC Samplers}

\begin{align*}
\pi_t(v_{1:t}, \theta) ~ &= ~ \frac{1}{Z_t} \gamma_t(v_{1:t}, \theta) \\
\d \gamma_t(v_{1:t}, \theta) ~ &= ~ L(x_{1:t} \mid v_{1:t}, \theta) \d \pi_0(v_{1:t}, \theta) \\
	&= ~ L(x_1 \mid v_1, \theta) \d \pi_0(v_1, \theta) \prod_{k=2}^t L(x_k \mid v_k, \theta) K(v_k \mid v_{k-1}, \theta) \\
Z_t ~ &= ~ \int L(x_{1:t} \mid v_{1:t}, \theta) \d \pi_0(v_{1:t}, \theta)
\end{align*}

\begin{align*}
W_t(v_{1:t}) ~ = ~ \frac{\d \gamma_t(v_{1:t})}{\d \nu_t(v_{1:t})}
\end{align*}

\begin{align*}
W_t(v_{1:t}, \theta) ~ := ~ \frac{\d \gamma_t(v_{1:t}, \theta)}{\d \nu_t(v_{1:t}, \theta)} ~ &= ~ \frac{\d \gamma_{t-1}(v_{1:t-1}, \theta)}{\d \nu_{t-1}(v_{1:t-1}, \theta)} \cdot \frac{\d \gamma_t(v_t \mid v_{1:t-1}, \theta)}{\d \nu_t(v_t \mid v_{1:t-1}, \theta)} \\
	&= ~ W_{t-1} \cdot \frac{\d \gamma_t(v_t \mid v_{t-1}) \d \gamma_t(x_t \mid x_{t-1}, v_t)}{\d \nu_t(v_t \mid v_{t-1}) \d \nu_t(x_t \mid x_{t-1}, v_t)}
\end{align*}

\subsection{Rao-Blackwell Particle Filtering}

\begin{theorem}[Rao-Blackwell]
\begin{equation*}
\E \left[ \left( \E \big[\hat{\theta}(X_{1:t}, \tau) \mid \tau(X_{1:t}) \big] - \theta \right)^2 \right] ~ \leq ~ \E \left[ \left( \hat{\theta}(X_{1:t}) - \theta \right)^2 \right]
\end{equation*}
\end{theorem}

\begin{lemma}[Student-t distribution]
If $\tau \sim \textrm{Gamma}(\nu/2, \nu/2)$ and $X \,|\, \tau ~ \sim ~ \mathcal{N}(0, \tau^{-1})$, then $X \sim t_\nu$.
\end{lemma}


