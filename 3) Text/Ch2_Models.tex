\chapter{Volatility Models}
\label{sec:models}

A volatility model is a model for the conditional standard deviation of the next value of a path given the past. Formally, we consider discrete $\R$-valued price processes with non-negative time indices, $S: \N_0 \to \R$, adapted to the filtration $\mathcal{F}_t = \sigma(\{S_{t'}, t' \leq t\})$. Based on the price process, define the \emph{log-returns} $(X_t)$ such that $X_t := \Delta \log S_{t-1} = \log \frac{S_t}{S_{t-1}}$. Conventionally, volatility models are constructed to model the latter.
inherently mean-zero, make no attempt at modeling expectation


\section{Deterministic Volatility Models}

In deterministic volatility models, the current volatility level is known given the filtration, the model, and the values of its parameters.
\begin{align*}
X_t ~ &= ~ \sigma_t Z_t \\
\sigma_t^2 ~ &= ~ f_\theta(t, S_{0:t-1})
\end{align*}

\begin{lemma}[Ito's Lemma]
Consider a stochastic process $(S_t)$ that satisfies the stochastic differential equation $\d S_t = \mu_t \d t + \sigma_t \d W_t$. Let $f(t, S_t)$ be a twice differentiable function mapping to $\R$. Then,
\begin{equation*}
\d f ~ = ~ \left( \partial_t f + \mu_t \partial_S f + \frac{1}{2} \sigma_t^2 \partial^2_S f \right) \d t + \sigma_t \partial_S f \d W_t
\end{equation*}
\end{lemma}

\begin{equation*}
\d \log S_t ~ = ~ \frac{\sigma_t}{S_t} \d \widetilde{W}_t
\end{equation*}

GBM
\begin{equation*}
\d S_t ~ = ~ \mu S_t \d t + \sigma S_t \d W_t
\end{equation*}
\begin{equation*}
X_t ~ = ~ \sigma Z_t
\end{equation*}

Ornstein-Uhlenbeck
\begin{equation*}
\d S_t ~ = ~ \kappa(\mu-S_t) \d t + \sigma \d W_t
\end{equation*}
\begin{equation*}
X_t ~ = ~ \frac{\sigma}{S_{t-1}} Z_t
\end{equation*}

GARCH
\begin{align*}
X_t ~ &= ~ \sigma_t Z_t \\
\sigma_t^2 ~ &= ~ \omega + \alpha X_{t-1}^2 + \beta \sigma_{t-1}^2
\end{align*}

\begin{equation*}
\sigma_t^2 ~ = ~ \omega \sum_{j=1}^t \beta^{j-1} + \alpha \sum_{j=1}^t \beta^{j-1} X_{t-j}^2 + \beta^{t-1} \sigma_1^2
\end{equation*}

E-GARCH
\begin{align*}
X_t ~ &= ~ \sigma_t Z_t \\
\log \sigma_t^2 ~ &= ~ \omega + \alpha \Big( |Z_{t-1}| - \E[|Z_{t-1}|] \Big) + \gamma \widetilde{Z}_{t-1} + \beta \log \sigma_t^2
\end{align*}

GJR-GARCH
\begin{align*}
X_t ~ &= ~ \sigma_t Z_t \\
\sigma_t^2 ~ &= ~ \omega + \Big( \alpha + \gamma 1(X_{t-1} > 0) \Big) X_{t-1}^2 + \beta \sigma_{t-1}^2
\end{align*}



\section{Stochastic Volatility Models}

Stochastic volatility models assume that volatility is a separate latent stochastic process.
\begin{align*}
X_t ~ &= ~ V_t Z_t \\
V_t ~ &= ~ f_\theta(t, V_{1:t-1}, \widetilde{Z}_{1:t})
\end{align*}
where $(\widetilde{Z}_t)$ is another standard white noise innovation process that may be correlated with $(Z_t)$, $\Cov(Z_t, \widetilde{Z}_t) = \rho$.

Stochastic volatility
\begin{align*}
X_t ~ &= ~ V_t Z_t \\
\log V_t^2 ~ &= ~ \omega(1-\alpha)  + \alpha \log V_{t-1}^2 + \xi \widetilde{Z}_t
\end{align*}

Heston model
\begin{align*}
\d S_t ~ &= ~ \mu S_t \d t + V_t S_t \d W_t \\
\d V_t^2 ~ &= ~ \kappa(\nu - V_t^2) \d t + \xi {V_t} \d \widetilde{W}_t
\end{align*}

\begin{align*}
X_t ~ &= ~ V_t Z_t \\
\log V_t ~ &= ~ \log V_{t-1}^2 + \kappa \left( \frac{\nu}{V_{t-1}^2} - 1 \right) - \frac{1}{2} \frac{\xi^2}{V_{t-1}^2} + \frac{\xi}{V_{t-1}} \widetilde{Z}_t
\end{align*}
$\Cov(Z_t, \widetilde{Z}_t) = \rho$

SABR model
\begin{align*}
\d S_t ~ &= ~ V_t S_t^\beta \d W_t \\
\d V_t^2 ~ &= ~ \alpha V_t^2 \d \widetilde{W}_t
\end{align*}

\begin{align*}
X_t ~ &= ~ V_t S_{t-1}^{\beta-1} Z_t \\
\log V_t^2 ~ &= ~ \alpha \widetilde{Z}_t
\end{align*}

Quintic Ornstein-Uhlenbeck
\begin{align*}
\d S_t ~ &= ~ V_t S_t \d W_t \\
\d V_t^2 ~ &= ~ \sqrt{\xi} \frac{p(X_t)}{\sqrt{\E[p(X_t)^2]}}, \quad p(x) = \alpha_0 + \alpha_1 x + \alpha_3 x^3 + \alpha_5 x^5 \\
\d X_t ~ &= ~ \varepsilon^{-1} (H - 1/2) X_t \d t + \varepsilon^{H-1/2} \d W_t
\end{align*}


\section{Neural Volatility Models}

Neural GARCH
\begin{align*}
X_t ~ &= ~ \sigma_t Z_t \\
\sigma_t^2 ~ &= ~ f_\phi(X_{t-1}, \sigma_{t-1}^2)
\end{align*}

Neural stochastic volatility
\begin{align*}
X_t ~  &= ~ V_t Z_t \\
V_t^2 ~ &= ~ f_\phi(V_{t-1}, \widetilde{Z}_t)
\end{align*}




\section{Local \& Path-Dependent Volatility}

The models introduced thus far all make one strong assumption: that the volatility today is uniquely determined by metrics of yesterday. In general, volatility may depend on the entire past path of observed as well as latent processes.

Consider first deterministic volatility models.

One approach is to handcraft a specific form in which path-dependence enters the model. For instance, \cite{Guyon22} propose a model wherein volatility is a simple affine function of a measure of the recent trend $\mathcal{T}$ and recent volatility $\Sigma$,
\begin{align*}
\sigma_t ~ &= ~ \beta_0 + \beta_1 \mathcal{T}_t + \beta_2 \Sigma_t
\end{align*}
where they define
\begin{align*}
\mathcal{T}_t ~ :=& ~ \sum_{t' \leq t} k_1(t-t') X_{t'} \\
\Sigma_t^2 ~ :=& ~ \sum_{t' \leq t} k_2(t-t') X_{t'}^2
\end{align*}
for some kernel functions $k_1, k_2: \R_+ \to \R_+$.

As in the previous section, neural networks allow to take a more data-driven approach also for path-dependent settings. \emph{Recurrent neural networks} (RNNs) maintain a hidden state $\pmb{h}_t$ which is recursively updated over time and hence are theoretically able to capture all relevant information of a path of observations.

A simple application of RNNs to deterministic volatility models might look as follows:
\begin{align*}
X_t ~ &= ~ \sigma_t Z_t \\
\sigma_t^2 ~ &= ~ f_\phi(\pmb{h}_t) \\
\pmb{h}_t ~ &= ~ g_\phi(\pmb{h}_{t-1}, X_{t-1})
\end{align*}

Similar applications have been proposed for stochastic volatility, e.g. by \citet{Luo18}.

Although RNNs are theoretically able to learn arbitrary functions on path spaces, they are limited by numerical issues in practice. Since the hidden state update $g_\phi(\cdot)$ is applied recursively, gradient information tends to be lost over time ("vanishing gradient problem"), and hence long-range dependencies are only very inefficiently learned. The more refined GRU or LSTM architectures are thus often preferred in practice. However, the idea of path-dependence is the same, and as is more illustrated in detail in Sect. \ref{sec:RC}, we will only take alternative approaches which merely approximately behave like neural models, anyway.

Denote by $(\widehat{X}_t) = ((t, X_t))$ the time-extended process of the log-returns.
\begin{align*}
X_t ~ &= ~ \sigma_t Z_t \\
\sigma_t^2 ~ &= ~ \pmb{\beta}^\top \Sig(\widehat{X}_{1:t-1}) + \beta_0
\end{align*}

\begin{align*}
X_t ~&= ~ V_t Z_t \\
V_t^2 ~ &= ~ \pmb{\beta}^\top \E \left[ \Sig(V_{1:t-1}) \mid X_{1:t-1} \right] + \beta_0
\end{align*}



\begin{table}[]
\caption{Models: Overview}
\begin{tabular}{|l|l|}
\hline
\textbf{Deterministic} & \textbf{Stochastic} \\ \hline
GBM                    & StochVol            \\ \hline
Ornstein-Uhlenbeck     & Heston              \\ \hline
GARCH                  & SABR                \\ \hline
E-GARCH                & Neural              \\ \hline
GJR-GARCH              & Signature           \\ \hline
Neural                 &                     \\ \hline
Signature              &                     \\ \hline
\end{tabular}
\end{table}


